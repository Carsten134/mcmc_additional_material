---
title: "MCMC Seminar Discoveries"
author: "Carsten Stahl"
date: "2024-06-12"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

## dependencies
library(rmcmc)
library(rbenchmark)
library(stats)
```

## MCMC Gibbs and Metropolis Hastings comparison
This document is concerned with the comparison of Metropolis Hastings and Gibbs sampling method. Both are MCMC algorithms, meaning the draw from a stationary Marcov chain, which stationary distribution is the desired distribution to draw from. 

### first test: Burn in time with normals
We are going to draw from a standard normal distribution with normal proposal distributions and normal marginal distribution s. 

```{r m.h-normal-data}

p <- function(x) {dnorm(x)}
q <- function(x, x_prime) {dnorm(x_prime, x)}
q_sample <- function(x) {rnorm(1,x)}

data_mh  = data.frame(x = metropolis_hastings(10, p, q, q_sample, 1000))
```

Let's plot the data:
```{r}
plot(data_mh$x, type="l")
```
Burn in time at around a 100. But we still see autocorrelation.

plot acf:
```{r}
acf(data_mh$x)
```
Significant correlation up to lag 10. Need i say more? 

plot the histogram:
```{r}
hist(data_mh$x)
```
We can see the tail at 10.

How about gibbs?

```{r}
cond <- list(x1 = function(x) {rnorm(1, x)},
     x2 = function(x) {rnorm(1,x)})

init <- list(x1 = 10,
             x2 = 10)

data_g <- gibbs(cond, init, 1000)
```

Plotting the data:
```{r}
plot(data_g$x2, type="l")
```
Much less autocorrelation, almost no burn in.

plot autocorrelation:
```{r}
acf(data_g$x1)
```
Correlation disappears at lag 1, truly white noise.

```{r}
hist(data_g$x2)
```

No tail, despite the initial value of 10.

### Test two: extreme initial values.

Let's try initial values of 100:
```{r}
data_mh$x2 <- metropolis_hastings(100, p, q, q_sample, 1000)
```
Runtime explodes, because the sample won't be picked up into the return.

On the other hand on gibbs:
```{r}
init <- list(x1 = 100000,
             x2 = 100000)

data_g_2 <- gibbs(cond, init, 1000)
```

No change in runtime, because there is no randomness in the algorithm

```{r}
plot(data_g_2$x2, type="l")
```
It is astonishing how fast this converges.

```{r}
plot(data_g_2$x1[3:1000], type="l")
```
Just left out the first 3 observations and already stationary marcov chain.


### Test 3: General runtime
scaling in n.
```{r}
benchmark_mh_with_n <- function(vals) {
  results <- data.frame(elapsed=rep(-1, length(vals)), n=vals)
  for (i in 1:length(results$elapsed)) {
    temp <- benchmark(metropolis_hastings(1,p, q, q_sample, vals[i]), replications = 5, columns = c("elapsed"))
    results$elapsed[i] <- temp$elapsed[1]
  }
  return(results)
}

```
Look at results:

```{r}
result <- benchmark_mh_with_n(c(10,100,200,500, 1000))
```

plotting the results:
```{r}
plot(result$n, result$elapsed, type="l")
```
Let's compare this to gibbs:
```{r}
benchmark_g_with_n <- function(vals) {
  results <- data.frame(elapsed=rep(-1, length(vals)), n=vals)
  for (i in 1:length(results$elapsed)) {
    temp <- benchmark(gibbs(cond, init, vals[i]), replications = 5, columns = c("elapsed"))
    results$elapsed[i] <- temp$elapsed[1]
  }
  return(results)
}
```

And run the test:
```{r}
results_g <- benchmark_g_with_n(c(10, 100, 200, 500, 1000))
results_g
```
plotting the results against each other:
```{r}

plot(result$n,result$elapsed, type = "n", xlim = c(1, 1000), ylim = c(0, max(result$elapsed)), 
     xlab = "X", ylab = "Y", main = "Gibbs vs M.H. runtime")
lines(result$n, result$elapsed, col="red")
lines(results_g$n, results_g$elapsed, col="green")
legend("topright", legend = c("Gibbs", "M.H"), 
       col = c("red", "green"), lty = 1)

``` 
The runtime isn't even comparable. This is because, once the M.H. get's caught up on a value, that yields low acceptance probability in all directions, it stayes there for many iterations. Furthermore we need to understand, that R is an interpreted language and not compiled. That leads to for-loops being very slow.




